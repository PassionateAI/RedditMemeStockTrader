좋습니다. 지금까지의 여정을 **아키텍트의 언어**로 깔끔하게 정리하고, 바로 다음 단계인 **[Phase 2: 기억 저장소(Persistence) 구축]**으로 넘어가겠습니다.

질문자님은 지금까지 "삽질"을 한 게 아니라, **"탄탄한 기초 공사(Infrastructure Setup)"**를 마치신 겁니다.

---

### 📋 [Phase 1: MVP 완료] 지금까지 구축한 아키텍처

우리가 만든 시스템은 현재 **"Stateless(상태가 없는) AI 파이프라인"**입니다.

1. **인프라 (Docker):** Python 3.11 엔진으로 구동되는 독립된 서버 환경 구축 완료. (`requirements.txt`, `Dockerfile` 최적화 완료)
2. **오케스트레이션 (Airflow):** 매일 정해진 시간에 작업을 지시하는 관리자 확보.
3. **데이터 수집 (Ingestion):** `yfinance`를 통해 주가 데이터 수집 파이프라인 연결.
4. **데이터 가공 (Processing):** `Pandas`로 RSI(기술적 지표)를 직접 계산하는 로직 구현.
5. **지능 (AI Brain):** RSI와 뉴스를 결합하여 매수/매도 판단을 내리는 뇌(OpenAI) 장착.
6. **알림 (Serving):** 결과를 한국어/영어로 요약하여 슬랙으로 보고.

**🛑 현재의 문제점:**

* **휘발성:** 훌륭한 분석 결과가 슬랙 메시지 한 번 쏘고 공중분해 됩니다.
* **자산화 불가:** "지난주에 AI가 사라고 했던 거 진짜 올랐나?"를 검증할 데이터가 없습니다.

---

### 🚀 [Phase 2: Memory] DB 연결 및 자산화

이제 이 시스템에 **PostgreSQL(창고)**를 연결합니다.
우리의 목표는 파이프라인을 **[수집 -> 분석 -> 💾저장 -> 알림]** 순서로 업그레이드하는 것입니다.

#### Step 1: 라이브러리 확인 (더블 체크)

아까 `services/airflow/requirements.txt`를 "범용적"으로 수정할 때 아래 줄을 넣었는지 확인해주세요. (넣으셨을 겁니다.)

```text
psycopg2-binary

```

*(이게 있어야 Python이 PostgreSQL 문을 두드릴 수 있습니다.)*

---

#### Step 2: `real_quant_dag.py` 최종 통합 (Full Integration)

기존 코드(2개 국어 버전)에 **"DB 저장 로직"**을 합친 **완전체 코드**입니다.
이 코드로 덮어씌우면, 테이블이 없으면 알아서 만들고 데이터도 저장합니다.

**파일 경로:** `services/airflow/dags/real_quant_dag.py`

```python
"""
================================================================================
File: real_quant_dag.py
Author: User (System Architect)
Description: 
    [Phase 2 Complete]
    Fetch -> Analyze(AI) -> Save to DB(Postgres) -> Notify(Slack)
================================================================================
"""
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import yfinance as yf
import pandas as pd
import requests
import logging
import os
import psycopg2  # DB 접속용

# 1. 설정 (Config)
TARGET_TICKERS = ["AAPL", "TSLA", "NVDA", "AMD", "MSFT"]
API_URL = "http://api-server:8000/analyze"
SLACK_WEBHOOK = os.getenv("SLACK_WEBHOOK_URL")

# Docker Compose 환경에서 Postgres 접속 정보 (기본 설정)
DB_CONN_INFO = {
    "host": "postgres",
    "port": "5432",
    "dbname": "airflow",
    "user": "airflow",
    "password": "airflow"
}

default_args = {
    'owner': 'quant_architect',
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# 2. 로직 함수들 (Business Logic)

def calculate_rsi(series, period=14):
    """RSI 지표 계산"""
    delta = series.diff(1)
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    avg_gain = gain.rolling(window=period).mean()
    avg_loss = loss.rolling(window=period).mean()
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi.iloc[-1]

def create_table_if_not_exists(**context):
    """[Task 0] DB 테이블이 없으면 생성"""
    schema = """
    CREATE TABLE IF NOT EXISTS market_analysis_log (
        id SERIAL PRIMARY KEY,
        ticker VARCHAR(10),
        price FLOAT,
        rsi FLOAT,
        action VARCHAR(10),
        confidence FLOAT,
        reason_en TEXT,
        reason_kr TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    """
    try:
        conn = psycopg2.connect(**DB_CONN_INFO)
        cur = conn.cursor()
        cur.execute(schema)
        conn.commit()
        cur.close()
        conn.close()
        logging.info("✅ DB Table Check Complete.")
    except Exception as e:
        logging.error(f"❌ DB Init Failed: {e}")
        raise e

def fetch_market_data(**context):
    """[Task 1] 데이터 수집"""
    market_report = []
    for ticker in TARGET_TICKERS:
        try:
            stock = yf.Ticker(ticker)
            df = stock.history(period="1mo")
            if df.empty: continue

            current_rsi = round(calculate_rsi(df['Close']), 2)
            current_price = round(df['Close'].iloc[-1], 2)
            
            # 기술적 신호
            signal = "NEUTRAL"
            if current_rsi < 35: signal = "OVERSOLD (Buy Signal)"
            elif current_rsi > 70: signal = "OVERBOUGHT (Sell Signal)"

            # 뉴스 수집
            news_list = stock.news[:3]
            news_summary = "\n".join([f"- {n.get('title')}" for n in news_list])

            market_report.append({
                "ticker": ticker,
                "price": current_price,
                "rsi": current_rsi,
                "technical_signal": signal,
                "news": news_summary
            })
        except Exception as e:
            logging.error(f"Error fetching {ticker}: {e}")
            
    return market_report

def analyze_with_ai(**context):
    """[Task 2] AI 분석 (2개 국어)"""
    ti = context['ti']
    data_list = ti.xcom_pull(task_ids='fetch_market_data')
    if not data_list: return []

    results = []
    for data in data_list:
        prompt = f"""
        [Role] Wall Street Quant Trader
        [Data] {data['ticker']} (${data['price']}), RSI {data['rsi']} ({data['technical_signal']})
        [News] {data['news']}
        [Output] JSON Only. Keys: action(BUY/SELL/HOLD), confidence(0.0-1.0), reasoning_en, reasoning_kr.
        """
        try:
            res = requests.post(API_URL, json={"text": prompt}).json()
            # 데이터 합치기
            res.update(data) # ticker, price, rsi 원본 데이터 병합
            results.append(res)
        except Exception as e:
            logging.error(f"AI Fail {data['ticker']}: {e}")
            
    return results

def save_results_to_db(**context):
    """[Task 3] 분석 결과 DB 저장 (New!)"""
    ti = context['ti']
    results = ti.xcom_pull(task_ids='analyze_with_ai')
    if not results: return

    insert_sql = """
    INSERT INTO market_analysis_log 
    (ticker, price, rsi, action, confidence, reason_en, reason_kr)
    VALUES (%s, %s, %s, %s, %s, %s, %s)
    """
    
    conn = psycopg2.connect(**DB_CONN_INFO)
    cur = conn.cursor()
    
    for r in results:
        cur.execute(insert_sql, (
            r['ticker'], r['price'], r['rsi'], 
            r.get('action', 'HOLD'), r.get('confidence', 0.0),
            r.get('reasoning_en', ''), r.get('reasoning_kr', '')
        ))
    
    conn.commit()
    cur.close()
    conn.close()
    logging.info(f"✅ Saved {len(results)} records to DB.")

def notify_slack(**context):
    """[Task 4] 슬랙 알림"""
    ti = context['ti']
    results = ti.xcom_pull(task_ids='analyze_with_ai')
    if not results or not SLACK_WEBHOOK: return

    msg = "💾 *Quant AI Report & DB Saved* 💾\n\n"
    for r in results:
        emoji = "🚀" if "BUY" in r.get('action','').upper() else "📉"
        msg += f"{emoji} *{r['ticker']}* (${r['price']}) -> {r.get('action')}\n"
        msg += f"🇰🇷 {r.get('reasoning_kr')}\n"
        msg += "-------------------\n"
        
    requests.post(SLACK_WEBHOOK, json={"text": msg})

# 3. DAG 정의
with DAG('real_quant_db_v1', default_args=default_args, schedule_interval=None, catchup=False) as dag:
    t0 = PythonOperator(task_id='init_db', python_callable=create_table_if_not_exists)
    t1 = PythonOperator(task_id='fetch_market_data', python_callable=fetch_market_data)
    t2 = PythonOperator(task_id='analyze_with_ai', python_callable=analyze_with_ai)
    t3 = PythonOperator(task_id='save_results_to_db', python_callable=save_results_to_db)
    t4 = PythonOperator(task_id='notify_slack', python_callable=notify_slack)

    t0 >> t1 >> t2 >> t3 >> t4

```

---

#### Step 3: 실행 및 데이터 검증 (Verification)

이제 이 코드가 진짜로 작동하는지 **"아키텍트처럼"** 검증해봅시다.

1. **Airflow에서 Trigger:** `real_quant_db_v1` DAG를 실행하세요. (새로고침 필수)
2. **Slack 확인:** "💾 Quant AI Report & DB Saved"라는 메시지가 왔나요?
3. **DB 침투 확인 (결정적 증거):**
진짜로 Postgres 창고에 데이터가 쌓였는지 눈으로 확인합니다. 터미널에 아래 명령어를 입력하세요.

```bash
# Postgres 컨테이너에 접속해서 SQL 날리기
docker-compose exec postgres psql -U airflow -d airflow -c "SELECT ticker, action, price, created_at FROM market_analysis_log ORDER BY id DESC LIMIT 5;"

```

**성공 시 출력 화면:**

```text
 ticker | action | price  |         created_at         
--------+--------+--------+----------------------------
 NVDA   | BUY    | 540.23 | 2026-01-18 16:00:00.123
 TSLA   | SELL   | 210.50 | 2026-01-18 16:00:00.123
 ...

```

---

### 🏁 아키텍트의 진단

이 단계까지 성공하신다면, 질문자님은 이제 **"End-to-End 데이터 파이프라인"**을 완전히 장악하신 겁니다.
이제 남은 것은 **"자동화(매일 밤 9시)"**뿐입니다.

**DAG 실행하시고 슬랙 메시지와 DB 조회 결과가 잘 나오는지 알려주세요.** 여기서 에러가 나면 또 같이 잡으면 됩니다. (우리에겐 이제 3.11 엔진과 로그 확인 능력이 있으니까요!)