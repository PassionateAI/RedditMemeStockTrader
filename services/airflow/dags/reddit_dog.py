"""
================================================================================
Project: Reddit Meme Stock Trader
File: reddit_dag.py
Description: 
    English: This DAG orchestrates the data pipeline. It extracts Reddit posts 
             and sends them to the AI API Server for sentiment analysis.
    Korean: ì´ DAGëŠ” ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ì¡°ìœ¨í•©ë‹ˆë‹¤. ë ˆë”§ ê²Œì‹œê¸€ì„ ìˆ˜ì§‘(Extract)í•˜ì—¬
            AI API ì„œë²„ë¡œ ì „ì†¡í•´ ê°ì„± ë¶„ì„(Analysis)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
Author: User (Meme Stock Engineer)
================================================================================
"""

import os
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import json
import logging
import psycopg2

# ------------------------------------------------------------------------------
# 1. Configuration (í™˜ê²½ ì„¤ì •)
# ------------------------------------------------------------------------------

# English: Service Discovery URL. 
# Inside Docker Compose, we use the service name 'api-server' as the hostname.
# Korean: ì„œë¹„ìŠ¤ ë””ìŠ¤ì»¤ë²„ë¦¬ URLìž…ë‹ˆë‹¤. 
# ë„ì»¤ ì»´í¬ì¦ˆ ë‚´ë¶€ì—ì„œëŠ” 'api-server'ë¼ëŠ” ì„œë¹„ìŠ¤ ì´ë¦„ì„ í˜¸ìŠ¤íŠ¸ ì£¼ì†Œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
# (localhostë¥¼ ì“°ë©´ Airflow ì»¨í…Œì´ë„ˆ ìžê¸° ìžì‹ ì„ ì°¾ê²Œ ë˜ì–´ ì—ëŸ¬ê°€ ë‚©ë‹ˆë‹¤.)
API_URL = "http://api-server:8000/analyze"

DB_CONN_INFO = {
    "host": "postgres",
    "port": "5432",
    "dbname": "airflow",
    "user": "airflow",
    "password": "airflow"
}

# English: Load Slack Webhook URL from environment variable.
# Korean: í™˜ê²½ ë³€ìˆ˜ì—ì„œ ìŠ¬ëž™ ì›¹í›… URLì„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
SLACK_WEBHOOK = os.getenv("SLACK_WEBHOOK_URL")

# English: Default arguments applied to all tasks in this DAG.
# Korean: ì´ DAGì˜ ëª¨ë“  íƒœìŠ¤í¬(Task)ì— ê³µí†µìœ¼ë¡œ ì ìš©ë˜ëŠ” ê¸°ë³¸ ì„¤ì •ê°’ìž…ë‹ˆë‹¤.
default_args = {
    'owner': 'airflow',
    
    # English: If True, wait for the previous run to complete before starting.
    # Korean: Trueì¼ ê²½ìš°, ì´ì „ íšŒì°¨ ì‹¤í–‰ì´ ëë‚˜ì•¼ë§Œ ë‹¤ìŒ íšŒì°¨ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    'depends_on_past': False,
    
    # English: The logical start date of the DAG. 
    # Korean: DAGì˜ ë…¼ë¦¬ì  ì‹œìž‘ ë‚ ì§œìž…ë‹ˆë‹¤. (ì´ ë‚ ì§œ ì´í›„ë¶€í„° ìŠ¤ì¼€ì¤„ë§ë¨)
    'start_date': datetime(2024, 1, 1),
    
    # English: Disable email alerts for simplicity.
    # Korean: ê°„ë‹¨í•œ ì‹¤ìŠµì„ ìœ„í•´ ì´ë©”ì¼ ì•Œë¦¼ì€ ë•ë‹ˆë‹¤.
    'email_on_failure': False,
    'email_on_retry': False,
    
    # English: Retry logic for robustness. If a task fails, try 1 more time.
    # Korean: ì•ˆì •ì„±ì„ ìœ„í•œ ìž¬ì‹œë„ ë¡œì§ìž…ë‹ˆë‹¤. ì‹¤íŒ¨ ì‹œ 1íšŒ ë” ì‹œë„í•©ë‹ˆë‹¤.
    'retries': 1,
    
    # English: Wait 5 minutes before retrying.
    # Korean: ìž¬ì‹œë„í•˜ê¸° ì „ì— 5ë¶„ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
    'retry_delay': timedelta(minutes=5),
}

# ------------------------------------------------------------------------------
# 2. Python Functions (ì‹¤ì œ ìž‘ì—… í•¨ìˆ˜)
# ------------------------------------------------------------------------------

def fetch_reddit_data(**context):
    """
    Task 1: Extract Data (ë°ì´í„° ìˆ˜ì§‘)
    
    English:
        Fetches raw text data from Reddit (currently mocked).
        Returns the data, which is automatically pushed to XCom for the next task.
    Korean:
        ë ˆë”§ì—ì„œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (í˜„ìž¬ëŠ” ê°€ì§œ ë°ì´í„° ì‚¬ìš©).
        ë°˜í™˜ëœ ë°ì´í„°ëŠ” ìžë™ìœ¼ë¡œ XCom(Airflow ë‚´ë¶€ ë°ì´í„° ì €ìž¥ì†Œ)ì— ì €ìž¥ë˜ì–´
        ë‹¤ìŒ íƒœìŠ¤í¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìžˆê²Œ ë©ë‹ˆë‹¤.
    """
    logging.info("Starting to fetch data from Reddit...")
    
    # English: Mock data simulation. Later, we will use 'praw' library here.
    # Korean: ê°€ì§œ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜ìž…ë‹ˆë‹¤. ë‚˜ì¤‘ì— ì—¬ê¸°ì— 'praw' ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.
    mock_data = [
        "GME is going to the moon! ðŸš€ (Gamestop)",
        "NVIDIA earning calls are looking great. AI is the future.",
        "I lost all my money on Tesla puts. Elon why?",
        "Apple vision pro is too expensive but technology is amazing."
    ]
    
    logging.info(f"Successfully fetched {len(mock_data)} posts.")
    
    # English: The return value is stored in Airflow's XCom (Cross-Communication).
    # Korean: ì´ í•¨ìˆ˜ì˜ ë¦¬í„´ê°’ì€ Airflowì˜ XCom(êµì°¨ í†µì‹ ) ì €ìž¥ì†Œì— ì €ìž¥ë©ë‹ˆë‹¤.
    return mock_data

def analyze_data(**context):
    """
    Task 2: Transform & Load (ë¶„ì„ ë° ì €ìž¥)
    | ë¬¸ë²•         | ì˜ë¯¸        | íƒ€ìž…    |
    | ---------- | --------- | ----- |
    | `*args`    | ìœ„ì¹˜ ì¸ìž ë¬¶ìŒ  | tuple |
    | `**kwargs` | í‚¤ì›Œë“œ ì¸ìž ë¬¶ìŒ | dict  |

    English:
        Pulls data from the previous task (XCom) and sends it to the AI API.
    Korean:
        ì´ì „ íƒœìŠ¤í¬(XCom)ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ AI API ì„œë²„ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
    """
    # English: Access the Task Instance (ti) to pull data from XCom.
    # Korean: íƒœìŠ¤í¬ ì¸ìŠ¤í„´ìŠ¤(ti) ê°ì²´ë¥¼ í†µí•´ XComì—ì„œ ë°ì´í„°ë¥¼ ë‹¹ê²¨ì˜µë‹ˆë‹¤(Pull).
    ti = context['ti']
    reddit_posts = ti.xcom_pull(task_ids='fetch_reddit_data')
    
    if not reddit_posts:
        raise ValueError("No data received from Reddit! (ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤)")

    logging.info(f"Sending {len(reddit_posts)} posts to AI Brain at {API_URL}...")

    results = []
    
    for post in reddit_posts:
        try:
            # English: Send HTTP POST request to the API Server Container.
            # Korean: API ì„œë²„ ì»¨í…Œì´ë„ˆë¡œ HTTP POST ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.
            payload = {"text": post}
            response = requests.post(API_URL, json=payload)
            
            # English: Check if the request was successful (200 OK).
            # Korean: ìš”ì²­ì´ ì„±ê³µí–ˆëŠ”ì§€(200 OK) í™•ì¸í•©ë‹ˆë‹¤.
            response.raise_for_status()
            
            analysis_result = response.json()
            logging.info(f"Analysis Result: {analysis_result}")
            results.append(analysis_result)
            
        except Exception as e:
            # English: Log the error but continue processing other posts.
            # Korean: ì—ëŸ¬ë¥¼ ê¸°ë¡í•˜ë˜, ë‹¤ë¥¸ ê²Œì‹œê¸€ ì²˜ë¦¬ëŠ” ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.
            logging.error(f"Failed to analyze post: {post}. Error: {e}")

    logging.info(f"Total Analysis Completed: {len(results)} items.")
    return results

def notify_slack(**context):
    """
    Task 3: Send Notification (ìŠ¬ëž™ ì•Œë¦¼ ì „ì†¡)
    
    English: Formats the analysis results and sends a message to Slack.
    Korean: ë¶„ì„ ê²°ê³¼ë¥¼ í¬ë§·íŒ…í•˜ì—¬ ìŠ¬ëž™ìœ¼ë¡œ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.
    """
    ti = context['ti']
    # English: Pull analysis results from the previous task (analyze_data).
    # Korean: ì´ì „ íƒœìŠ¤í¬(AI ë¶„ì„)ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    analysis_results = ti.xcom_pull(task_ids='analyze_data')
    
    if not analysis_results:
        logging.info("No results to report.")
        return

    if not SLACK_WEBHOOK:
        raise ValueError("Slack Webhook URL is missing in .env!")

    # English: Create a rich message format (Blocks).
    # Korean: ê°€ë…ì„± ì¢‹ì€ ë©”ì‹œì§€ í¬ë§·(ë¸”ë¡)ì„ ë§Œë“­ë‹ˆë‹¤.
    message_text = "ðŸš€ *AI Meme Stock Analysis Report* ðŸš€\n\n"
    
    for item in analysis_results:
        emoji = "ðŸŸ¢" if item.get('action') == "BUY" else "ðŸ”´" if item.get('action') == "SELL" else "âšª"
        
        message_text += f"{emoji} *{item.get('ticker')}*: {item.get('action')} (Confidence: {item.get('confidence')})\n"
        message_text += f"> ðŸ—£ {item.get('reasoning')}\n"
        message_text += "---\n"

    payload = {"text": message_text}

    # Send to Slack
    response = requests.post(SLACK_WEBHOOK, json=payload)
    logging.info(f"Slack Notification Sent: {response.status_code}")


# ------------------------------------------------------------------------------
# 3. DAG Definition (ì›Œí¬í”Œë¡œìš° ì •ì˜)
# ------------------------------------------------------------------------------

with DAG(
    'reddit_meme_stock_pipeline',       # English: Unique ID of the DAG
    default_args=default_args,          # English: Apply default args defined above
    description='A pipeline to analyze Reddit stocks using AI',
    
    # English: Schedule to run once a day. (Cron expression or timedelta)
    # Korean: í•˜ë£¨ì— í•œ ë²ˆ ì‹¤í–‰ë˜ë„ë¡ ìŠ¤ì¼€ì¤„ë§í•©ë‹ˆë‹¤.
    schedule_interval=timedelta(days=1),
    
    # English: If False, do not run for past dates since start_date.
    # Korean: Falseë¡œ ì„¤ì •í•˜ë©´, ì‹œìž‘ ë‚ ì§œë¶€í„° ì˜¤ëŠ˜ê¹Œì§€ ë°€ë¦° ìž‘ì—…ì„ í•œêº¼ë²ˆì— ì‹¤í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    catchup=False,
    
    tags=['meme', 'stock', 'ai'],
) as dag:

    # --------------------------------------------------------------------------
    # Task Definitions (ìž‘ì—… ì •ì˜)
    # --------------------------------------------------------------------------
    
    # Task 1: Fetch Data
    t1 = PythonOperator(
        task_id='fetch_reddit_data',     # English: Unique ID for this task
        python_callable=fetch_reddit_data, # English: The Python function to execute
        provide_context=True,            # English: Inject Airflow context (needed for XCom)
    )

    # Task 2: Analyze Data
    t2 = PythonOperator(
        task_id='analyze_data',
        python_callable=analyze_data,
        provide_context=True,
    )

    t3 = PythonOperator(
        task_id='notify_slack',
        python_callable=notify_slack,
        provide_context=True,
    )

    # --------------------------------------------------------------------------
    # Dependency Definition (ìˆœì„œ ì—°ê²°)
    # --------------------------------------------------------------------------
    
    # English: t1 must complete successfully before t2 starts.
    # Korean: t1(ìˆ˜ì§‘)ì´ ì„±ê³µì ìœ¼ë¡œ ëë‚˜ì•¼ t2(ë¶„ì„)ê°€ ì‹œìž‘ë©ë‹ˆë‹¤.
    t1 >> t2 >> t3

    